{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vaegan_working_original.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "i7Yf9JmJPldV",
        "colab_type": "code",
        "outputId": "d3db700e-002c-4464-b0ba-ad4df075731b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ow4bOJW7AihP",
        "colab_type": "code",
        "outputId": "e0eab7f7-c056-432b-e4d0-a16a9bf097b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "  Using cached https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPr5SiU9PUFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy\n",
        "import argparse\n",
        "numpy.random.seed(8)\n",
        "torch.manual_seed(8)\n",
        "torch.cuda.manual_seed(8)\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import RMSprop,Adam,SGD\n",
        "from torch.optim.lr_scheduler import ExponentialLR,MultiStepLR\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3Dehf-zPUFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy\n",
        "\n",
        "# encoder block (used in encoder and discriminator)\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        # convolution to halve the dimensions\n",
        "        self.conv = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=5, padding=2, stride=2,\n",
        "                              bias=False)\n",
        "        self.bn = nn.BatchNorm2d(num_features=channel_out, momentum=0.9)\n",
        "\n",
        "    def forward(self, ten, out=False,t = False):\n",
        "        # here we want to be able to take an intermediate output for reconstruction error\n",
        "        #print(\"Input size: \"),\n",
        "        #print(ten.shape)\n",
        "        #print(\"Encoder block size: \",)\n",
        "        if out:\n",
        "            ten = self.conv(ten)\n",
        "            #print(ten.size())\n",
        "            ten_out = ten\n",
        "            ten = self.bn(ten)\n",
        "            ten = F.relu(ten, False)\n",
        "            return ten, ten_out\n",
        "        else:\n",
        "            ten = self.conv(ten)\n",
        "            #print(ten.size())\n",
        "            ten = self.bn(ten)\n",
        "            ten = F.relu(ten, True)\n",
        "            return ten\n",
        "\n",
        "\n",
        "# decoder block (used in the decoder)\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # transpose convolution to double the dimensions\n",
        "        self.conv = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=5, padding=2, stride=2, output_padding=1,\n",
        "                                       bias=False)\n",
        "        self.bn = nn.BatchNorm2d(channel_out, momentum=0.9)\n",
        "\n",
        "    def forward(self, ten):\n",
        "        #print(\"Input size: \"),\n",
        "        #print(ten.shape)\n",
        "        #print(\"Decoder block size: \",)\n",
        "        ten = self.conv(ten)\n",
        "        #print(ten.size())\n",
        "        ten = self.bn(ten)\n",
        "        ten = F.relu(ten, True)\n",
        "        return ten\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channel_in=3, z_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.size = channel_in\n",
        "        layers_list = []\n",
        "        # the first time 3->64, for every other double the channel size\n",
        "        for i in range(3):\n",
        "            if i == 0:\n",
        "                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=64))\n",
        "                self.size = 64\n",
        "            else:\n",
        "                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=self.size * 2))\n",
        "                self.size *= 2\n",
        "\n",
        "        # final shape Bx256x8x8\n",
        "        self.conv = nn.Sequential(*layers_list)\n",
        "        self.fc = nn.Sequential(nn.Linear(in_features=8 * 8 * self.size, out_features=1024, bias=False),\n",
        "                                nn.BatchNorm1d(num_features=1024,momentum=0.9),\n",
        "                                nn.ReLU(True))\n",
        "        # two linear to get the mu vector and the diagonal of the log_variance\n",
        "        self.l_mu = nn.Linear(in_features=1024, out_features=z_size)\n",
        "        self.l_var = nn.Linear(in_features=1024, out_features=z_size)\n",
        "\n",
        "    def forward(self, ten):\n",
        "        ten = self.conv(ten)\n",
        "        ten = ten.view(len(ten), -1)\n",
        "        ten = self.fc(ten)\n",
        "        mu = self.l_mu(ten)\n",
        "        logvar = self.l_var(ten)\n",
        "        return mu, logvar\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Encoder, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_size, size):\n",
        "        super(Decoder, self).__init__()\n",
        "        # start from B*z_size\n",
        "        self.fc = nn.Sequential(nn.Linear(in_features=z_size, out_features=8 * 8 * size, bias=False),\n",
        "                                nn.BatchNorm1d(num_features=8 * 8 * size,momentum=0.9),\n",
        "                                nn.ReLU(True))\n",
        "        self.size = size\n",
        "        layers_list = []\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size))\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//2))\n",
        "        self.size = self.size//2\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//4))\n",
        "        self.size = self.size//4\n",
        "        # final conv to get 3 channels and tanh layer\n",
        "        layers_list.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.size, out_channels=3, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        ))\n",
        "\n",
        "        self.conv = nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, ten):\n",
        "\n",
        "        ten = self.fc(ten)\n",
        "        ten = ten.view(len(ten), -1, 8, 8)\n",
        "        ten = self.conv(ten)\n",
        "        return ten\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Decoder, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channel_in=3,recon_level=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.size = channel_in\n",
        "        self.recon_levl = recon_level\n",
        "        # module list because we need need to extract an intermediate output\n",
        "        self.conv = nn.ModuleList()\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        self.size = 32\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=128))\n",
        "        self.size = 128\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=256))\n",
        "        self.size = 256\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=256))\n",
        "        # final fc to get the score (real or fake)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=8 * 8 * self.size, out_features=512, bias=False),\n",
        "            nn.BatchNorm1d(num_features=512,momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=1),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, ten,ten_original,ten_sampled):\n",
        "\n",
        "        ten = torch.cat((ten, ten_original,ten_sampled), 0)\n",
        "\n",
        "        for i, lay in enumerate(self.conv):\n",
        "            # we take the 9th layer as one of the outputs\n",
        "            if i == self.recon_levl:\n",
        "                ten, layer_ten = lay(ten, True)\n",
        "                # we need the layer representations just for the original and reconstructed,\n",
        "                # flatten, because it's a convolutional shape\n",
        "                layer_ten = layer_ten.view(len(layer_ten), -1)\n",
        "            else:\n",
        "                ten = lay(ten)\n",
        "\n",
        "        ten = ten.view(len(ten), -1)\n",
        "        ten = self.fc(ten)\n",
        "        return layer_ten,torch.sigmoid(ten)\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Discriminator, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class VaeGan(nn.Module):\n",
        "    def __init__(self,z_size=128,recon_level=3):\n",
        "        super(VaeGan, self).__init__()\n",
        "        # latent space size\n",
        "        self.z_size = z_size\n",
        "        self.encoder = Encoder(z_size=self.z_size)\n",
        "        self.decoder = Decoder(z_size=self.z_size, size=self.encoder.size)\n",
        "        self.discriminator = Discriminator(channel_in=3,recon_level=recon_level)\n",
        "        # self-defined function to init the parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        # just explore the network, find every weight and bias matrix and fill it\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
        "                if hasattr(m, \"weight\") and m.weight is not None and m.weight.requires_grad:\n",
        "                    #init as original implementation\n",
        "                    scale = 1.0/numpy.sqrt(numpy.prod(m.weight.shape[1:]))\n",
        "                    scale /=numpy.sqrt(3)\n",
        "                    #nn.init.xavier_normal(m.weight,1)\n",
        "                    #nn.init.constant(m.weight,0.005)\n",
        "                    nn.init.uniform(m.weight,-scale,scale)\n",
        "                if hasattr(m, \"bias\") and m.bias is not None and m.bias.requires_grad:\n",
        "                    nn.init.constant(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, ten, gen_size=10):\n",
        "        if self.training:\n",
        "            # save the original images\n",
        "            ten_original = ten\n",
        "            # encode\n",
        "            mus, log_variances = self.encoder(ten)\n",
        "            # we need the true variances, not the log one\n",
        "            variances = torch.exp(log_variances * 0.5)\n",
        "            # sample from a gaussian\n",
        "\n",
        "            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n",
        "            # shift and scale using the means and variances\n",
        "\n",
        "            ten = ten_from_normal * variances + mus\n",
        "            # decode the tensor\n",
        "            ten = self.decoder(ten)\n",
        "            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n",
        "            ten_from_normal = self.decoder(ten_from_normal)\n",
        "            #discriminator\n",
        "            ten_layer,ten_class = self.discriminator(ten,ten_original,ten_from_normal)\n",
        "\n",
        "            return ten, ten_class, ten_layer, mus, log_variances\n",
        "\n",
        "        else:\n",
        "            if ten is None:\n",
        "                # just sample and decode\n",
        "\n",
        "                ten = Variable(torch.randn(gen_size, self.z_size).cuda(), requires_grad=False)\n",
        "                ten = self.decoder(ten)\n",
        "            else:\n",
        "                mus, log_variances = self.encoder(ten)\n",
        "                # we need the true variances, not the log one\n",
        "                variances = torch.exp(log_variances * 0.5)\n",
        "                # sample from a gaussian\n",
        "\n",
        "                ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=False)\n",
        "                # shift and scale using the means and variances\n",
        "                ten = ten_from_normal * variances + mus\n",
        "                # decode the tensor\n",
        "                ten = self.decoder(ten)\n",
        "            return ten\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(VaeGan, self).__call__(*args, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(ten_original, ten_predicted, layer_original, layer_predicted,layer_sampled, labels_original,\n",
        "             labels_predicted,labels_sampled, mus, variances):\n",
        "        \"\"\"\n",
        "\n",
        "        :param ten_original: original images\n",
        "        :param ten_predicted:  predicted images (output of the decoder)\n",
        "        :param layer_original:  intermediate layer for original (intermediate output of the discriminator)\n",
        "        :param layer_predicted: intermediate layer for reconstructed (intermediate output of the discriminator)\n",
        "        :param labels_original: labels for original (output of the discriminator)\n",
        "        :param labels_predicted: labels for reconstructed (output of the discriminator)\n",
        "        :param labels_sampled: labels for sampled from gaussian (0,1) (output of the discriminator)\n",
        "        :param mus: tensor of means\n",
        "        :param variances: tensor of diagonals of log_variances\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # reconstruction error, not used for the loss but useful to evaluate quality\n",
        "        nle = 0.5*(ten_original.view(len(ten_original), -1) - ten_predicted.view(len(ten_predicted), -1)) ** 2\n",
        "        # kl-divergence\n",
        "        kl = -0.5 * torch.sum(-variances.exp() - torch.pow(mus,2) + variances + 1, 1)\n",
        "        # mse between intermediate layers for both\n",
        "        mse_1 = torch.sum(0.5*(layer_original - layer_predicted) ** 2, 1)\n",
        "        mse_2 = torch.sum(0.5*(layer_original - layer_sampled) ** 2, 1)\n",
        "        # bce for decoder and discriminator for original,sampled and reconstructed\n",
        "        # the only excluded is the bce_gen_original\n",
        "\n",
        "        bce_dis_original = -torch.log(labels_original + 1e-3)\n",
        "        bce_dis_sampled = -torch.log(1 - labels_sampled + 1e-3)\n",
        "        bce_dis_recon = -torch.log(1 - labels_predicted+ 1e-3)\n",
        "\n",
        "        #bce_gen_original = -torch.log(1-labels_original + 1e-3)\n",
        "        bce_gen_sampled = -torch.log(labels_sampled + 1e-3)\n",
        "        bce_gen_recon = -torch.log(labels_predicted+ 1e-3)\n",
        "        '''\n",
        "        \n",
        "\n",
        "        bce_gen_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n",
        "                                         Variable(torch.ones_like(labels_predicted.data).cuda(), requires_grad=False))\n",
        "        bce_gen_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n",
        "                                       Variable(torch.ones_like(labels_sampled.data).cuda(), requires_grad=False))\n",
        "        bce_dis_original = nn.BCEWithLogitsLoss(size_average=False)(labels_original,\n",
        "                                        Variable(torch.ones_like(labels_original.data).cuda(), requires_grad=False))\n",
        "        bce_dis_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n",
        "                                         Variable(torch.zeros_like(labels_predicted.data).cuda(), requires_grad=False))\n",
        "        bce_dis_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n",
        "                                       Variable(torch.zeros_like(labels_sampled.data).cuda(), requires_grad=False))\n",
        "        '''\n",
        "        return nle, kl, mse_1,mse_2,\\\n",
        "               bce_dis_original, bce_dis_sampled,bce_dis_recon,bce_gen_sampled,bce_gen_recon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gf8WHPJ5PUF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# just a class to store a rolling average\n",
        "# useful to log to TB\n",
        "class RollingMeasure(object):\n",
        "    def __init__(self):\n",
        "        self.measure = 0.0\n",
        "        self.iter = 0\n",
        "    def __call__(self, measure):\n",
        "        # passo nuovo valore e ottengo average\n",
        "        # se first call inizializzo\n",
        "        if self.iter == 0:\n",
        "            self.measure = measure\n",
        "        else:\n",
        "            self.measure = (1.0 / self.iter * measure) + (1 - 1.0 / self.iter) * self.measure\n",
        "        self.iter += 1\n",
        "        return self.measure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4kbeKw7PUF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST(Dataset):\n",
        "    def __init__(self,X,Y):\n",
        "        self.X=X\n",
        "        self.Y=Y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self,index):\n",
        "        return self.X[index],self.Y[index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LB8t98-fFuKW",
        "colab_type": "code",
        "outputId": "ed3fa378-a0d2-4f8f-c39c-fe9f2c1f4b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yn6yIaXOMVeA",
        "colab_type": "code",
        "outputId": "d81d4c52-5b16-4caf-9f31-17930ef5d1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install Pillow==4.0.0\n",
        "!pip install PIL\n",
        "!pip install image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KJjKaaOtDNyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import io\n",
        "data = zipfile.ZipFile('mnist_png.zip', 'r')\n",
        "data.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ31ZsKuPUGG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from skimage import filters,transform\n",
        "\n",
        "train_x_data=[]\n",
        "train_y_data=[]\n",
        "\n",
        "count=0\n",
        "\n",
        "for number in range(10):\n",
        "    file_directory=\"mnist_png/mnist_png/training/\"+str(number)+\"/\"\n",
        "    count=0\n",
        "    for each_file in os.listdir(file_directory):\n",
        "        if(count>500):\n",
        "          break\n",
        "        count+=1\n",
        "        image=cv2.imread(file_directory+str(each_file))\n",
        "        image=numpy.float64(image)\n",
        "        image = cv2.resize(image, (64,64))\n",
        "        h=image.shape[0]\n",
        "        w=image.shape[1]\n",
        "        for i in range(h):\n",
        "          for j in range(w):\n",
        "            image[i][j]=image[i][j]/255.0\n",
        "        #image = numpy.stack((image,)*3, axis=-1)\n",
        "        train_x_data.append(image)\n",
        "        train_y_data.append(number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGcMKgWRe84g",
        "colab_type": "code",
        "outputId": "aa4ecd50-9d30-45c1-c392-2040b3196b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(train_x_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3bOt6ikvRvro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset=MNIST(train_x_data, train_y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbZ3QzTCqP6V",
        "colab_type": "code",
        "outputId": "5d7b98ac-a1fa-476b-f210-5d0801ee10a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "z_size=128\n",
        "recon_level=3\n",
        "net = VaeGan(z_size=z_size,recon_level=recon_level).cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:195: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rGmZDUqEPUGL",
        "colab_type": "code",
        "outputId": "2bdb4b3f-22c5-4f58-d7e0-778e31146e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2414
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tensorboardX\n",
        "from tensorboardX import SummaryWriter\n",
        "import cv2\n",
        "from skimage import filters,transform\n",
        "#import progressbar\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "writer=SummaryWriter()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    dataset = MNIST(train_x_data, train_y_data)\n",
        "\n",
        "    z_size = 128\n",
        "    recon_level = 3\n",
        "    decay_mse = 1.0\n",
        "    decay_margin = 1.0\n",
        "    n_epochs = 7\n",
        "    lambda_mse = 1e-6\n",
        "    lr = 0.001\n",
        "    decay_lr = 0.75\n",
        "    decay_equilibrium = 1.0\n",
        "    \n",
        "    # DATASET\n",
        "    \n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    dataloader_test = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=1)\n",
        "    \n",
        "    #margin and equilibirum\n",
        "    margin = 0.35\n",
        "    equilibrium = 0.68\n",
        "    mse_lambda = 1.0\n",
        "    # OPTIM-LOSS\n",
        "    # an optimizer for each of the sub-networks, so we can selectively backprop\n",
        "    optimizer_encoder = RMSprop(params=net.encoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "    #lr_encoder = MultiStepLR(optimizer_encoder,milestones=[2],gamma=1)\n",
        "    lr_encoder = ExponentialLR(optimizer_encoder, gamma=decay_lr)\n",
        "    optimizer_decoder = RMSprop(params=net.decoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "    lr_decoder = ExponentialLR(optimizer_decoder, gamma=decay_lr)\n",
        "    #lr_decoder = MultiStepLR(optimizer_decoder,milestones=[2],gamma=1)\n",
        "    optimizer_discriminator = RMSprop(params=net.discriminator.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "    lr_discriminator = ExponentialLR(optimizer_discriminator, gamma=decay_lr)\n",
        "    #lr_discriminator = MultiStepLR(optimizer_discriminator,milestones=[2],gamma=1)\n",
        "\n",
        "    batch_number = len(dataloader)\n",
        "    step_index = 0\n",
        "    for i in range(n_epochs):\n",
        "        #progress = progressbar.ProgressBar(min_value=0, max_value=batch_number, initial_value=0, widgets=widgets).start()\n",
        "        # reset rolling average\n",
        "        print(\"Epoch number: \",i)\n",
        "        loss_nle_mean = RollingMeasure()\n",
        "        loss_encoder_mean = RollingMeasure()\n",
        "        loss_decoder_mean = RollingMeasure()\n",
        "        loss_discriminator_mean = RollingMeasure()\n",
        "        loss_reconstruction_layer_mean = RollingMeasure()\n",
        "        loss_kld_mean = RollingMeasure()\n",
        "        gan_gen_eq_mean = RollingMeasure()\n",
        "        gan_dis_eq_mean = RollingMeasure()\n",
        "        #print(\"LR:{}\".format(lr_encoder.get_lr()))\n",
        "\n",
        "        # for each batch\n",
        "        count=0\n",
        "        for j, (data_batch,target_batch) in enumerate(dataloader):\n",
        "            # set to train mode\n",
        "            count+=len(data_batch)\n",
        "            if(count%3000==0):\n",
        "              print(\"count: \",count)\n",
        "            train_batch = len(data_batch)\n",
        "            data_batch=data_batch.transpose(2,3).transpose(1,2)\n",
        "            net.train()\n",
        "            # target and input are the same images\n",
        "            data_target = Variable(data_batch, requires_grad=False).float().cuda()\n",
        "            data_in = Variable(data_batch, requires_grad=False).float().cuda()\n",
        "\n",
        "            # get output\n",
        "            out, out_labels, out_layer, mus, variances = net(data_in)\n",
        "            \n",
        "            # split so we can get the different parts\n",
        "            out_layer_predicted = out_layer[:train_batch]\n",
        "            out_layer_original = out_layer[train_batch:-train_batch]\n",
        "            out_layer_sampled = out_layer[-train_batch:]\n",
        "            #labels\n",
        "            out_labels_predicted = out_labels[:train_batch]\n",
        "            out_labels_original = out_labels[train_batch:-train_batch]\n",
        "            out_labels_sampled = out_labels[-train_batch:]\n",
        "            # loss, nothing special here\n",
        "            nle_value, kl_value, mse_value_1,mse_value_2, bce_dis_original_value, bce_dis_sampled_value, \\\n",
        "            bce_dis_predicted_value,bce_gen_sampled_value,bce_gen_predicted_value= VaeGan.loss(data_target, out, out_layer_original,\n",
        "                                                                         out_layer_predicted,out_layer_sampled, out_labels_original,\n",
        "                                                                          out_labels_predicted,out_labels_sampled, mus,\n",
        "                                                                         variances)\n",
        "            # THIS IS THE MOST IMPORTANT PART OF THE CODE\n",
        "            loss_encoder = torch.sum(kl_value)+torch.sum(mse_value_1)+torch.sum(mse_value_2)\n",
        "            loss_discriminator = torch.sum(bce_dis_original_value) + torch.sum(bce_dis_sampled_value)+ torch.sum(bce_dis_predicted_value)\n",
        "            loss_decoder = torch.sum(bce_gen_sampled_value) + torch.sum(bce_gen_predicted_value)\n",
        "            loss_decoder = torch.sum(lambda_mse/2 * mse_value_1)+ torch.sum(lambda_mse/2 * mse_value_2) + (1.0 - lambda_mse) * loss_decoder\n",
        "\n",
        "            # register mean values of the losses for logging\n",
        "            loss_nle_mean(torch.mean(nle_value).data.cpu().numpy())\n",
        "            loss_discriminator_mean((torch.mean(bce_dis_original_value) + torch.mean(bce_dis_sampled_value)).data.cpu().numpy())\n",
        "            loss_decoder_mean((torch.mean(lambda_mse * mse_value_1/2)+torch.mean(lambda_mse * mse_value_2/2) + (1 - lambda_mse) * (torch.mean(bce_gen_predicted_value) + torch.mean(bce_gen_sampled_value))).data.cpu().numpy())\n",
        "\n",
        "            loss_encoder_mean((torch.mean(kl_value) + torch.mean(mse_value_1)+ torch.mean(mse_value_2)).data.cpu().numpy())\n",
        "            loss_reconstruction_layer_mean((torch.mean(mse_value_1)+torch.mean(mse_value_2)).data.cpu().numpy())\n",
        "            loss_kld_mean(torch.mean(kl_value).data.cpu().numpy())\n",
        "            # selectively disable the decoder of the discriminator if they are unbalanced\n",
        "            train_dis = True\n",
        "            train_dec = True\n",
        "            if torch.mean(bce_dis_original_value).item() < equilibrium-margin or torch.mean(bce_dis_sampled_value).item() < equilibrium-margin:\n",
        "                train_dis = False\n",
        "            if torch.mean(bce_dis_original_value).item() > equilibrium+margin or torch.mean(bce_dis_sampled_value).item() > equilibrium+margin:\n",
        "                train_dec = False\n",
        "            if train_dec is False and train_dis is False:\n",
        "                train_dis = True\n",
        "                train_dec = True\n",
        "\n",
        "            #aggiungo log\n",
        "            if train_dis:\n",
        "                gan_dis_eq_mean(1.0)\n",
        "            else:\n",
        "                gan_dis_eq_mean(0.0)\n",
        "\n",
        "            if train_dec:\n",
        "                gan_gen_eq_mean(1.0)\n",
        "            else:\n",
        "                gan_gen_eq_mean(0.0)\n",
        "\n",
        "            # BACKPROP\n",
        "            # clean grads\n",
        "            net.zero_grad()\n",
        "            # encoder\n",
        "            loss_encoder.backward(retain_graph=True)\n",
        "            # someone likes to clamp the grad here\n",
        "            #[p.grad.data.clamp_(-1,1) for p in net.encoder.parameters()]\n",
        "            # update parameters\n",
        "            optimizer_encoder.step()\n",
        "            # clean others, so they are not afflicted by encoder loss\n",
        "            net.zero_grad()\n",
        "            #decoder\n",
        "            if train_dec:\n",
        "                loss_decoder.backward(retain_graph=True)\n",
        "                #[p.grad.data.clamp_(-1,1) for p in net.decoder.parameters()]\n",
        "                optimizer_decoder.step()\n",
        "                #clean the discriminator\n",
        "                net.discriminator.zero_grad()\n",
        "            #discriminator\n",
        "            if train_dis:\n",
        "                loss_discriminator.backward()\n",
        "                #[p.grad.data.clamp_(-1,1) for p in net.discriminator.parameters()]\n",
        "                optimizer_discriminator.step()\n",
        "        \n",
        "        lr_encoder.step()\n",
        "        lr_decoder.step()\n",
        "        lr_discriminator.step()\n",
        "        margin *=decay_margin\n",
        "        equilibrium *=decay_equilibrium\n",
        "        #margin non puo essere piu alto di equilibrium\n",
        "        if margin > equilibrium:\n",
        "            equilibrium = margin\n",
        "        lambda_mse *=decay_mse\n",
        "        if lambda_mse > 1:\n",
        "            lambda_mse=1\n",
        "        #progress.finish()\n",
        "        \n",
        "        writer.add_scalar('loss_encoder', loss_encoder_mean.measure, step_index)\n",
        "        writer.add_scalar('loss_decoder', loss_decoder_mean.measure, step_index)\n",
        "        writer.add_scalar('loss_discriminator', loss_discriminator_mean.measure, step_index)\n",
        "        writer.add_scalar('loss_reconstruction', loss_nle_mean.measure, step_index)\n",
        "        writer.add_scalar('loss_kld',loss_kld_mean.measure,step_index)\n",
        "        writer.add_scalar('gan_gen',gan_gen_eq_mean.measure,step_index)\n",
        "        writer.add_scalar('gan_dis',gan_dis_eq_mean.measure,step_index)\n",
        "        \n",
        "        testloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4)\n",
        "    \n",
        "        for j, (data_batch,target_batch) in enumerate(testloader):\n",
        "          if(j>1):\n",
        "            break\n",
        "          train_batch = len(data_batch)\n",
        "          data_batch=data_batch.transpose(2,3).transpose(1,2)\n",
        "          data_target = Variable(target_batch, requires_grad=False).float().cuda()\n",
        "          data_in = Variable(data_batch, requires_grad=False).float().cuda()\n",
        "          out, out_labels, out_layer, mus, variances = net(data_in)\n",
        "          \n",
        "        input_images=data_in.data.cpu().numpy()\n",
        "        output_images=out.data.cpu().numpy()\n",
        "        print(input_images[0][0])\n",
        "        print(output_images[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number:  0\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0.00140453 0.00140457 0.00140456 ... 0.00140452 0.00140452 0.00140452]\n",
            " [0.00140449 0.00140455 0.00140449 ... 0.00140452 0.00140452 0.00140452]\n",
            " [0.00140453 0.00140457 0.00140456 ... 0.00140452 0.00140452 0.00140452]\n",
            " ...\n",
            " [0.00140442 0.00140442 0.00140445 ... 0.00140452 0.00140452 0.00140452]\n",
            " [0.00140439 0.00140442 0.00140441 ... 0.00140452 0.00140452 0.00140452]\n",
            " [0.00140441 0.00140439 0.00140438 ... 0.00140452 0.00140452 0.00140452]]\n",
            "Epoch number:  1\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[-0.00076362 -0.00076354 -0.00076361 ... -0.00076365 -0.00076365\n",
            "  -0.00076365]\n",
            " [-0.00076356 -0.00076353 -0.00076363 ... -0.00076365 -0.00076365\n",
            "  -0.00076365]\n",
            " [-0.0007636  -0.00076358 -0.00076361 ... -0.00074255 -0.00074498\n",
            "  -0.00076365]\n",
            " ...\n",
            " [-0.00076365 -0.00076365 -0.00076365 ... -0.00066793 -0.00080606\n",
            "  -0.00076365]\n",
            " [-0.00076365 -0.00076365 -0.00076365 ... -0.00071811 -0.00072099\n",
            "  -0.00076365]\n",
            " [-0.00076365 -0.00076365 -0.00076365 ... -0.00071993 -0.00077377\n",
            "  -0.00076365]]\n",
            "Epoch number:  2\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[ 1.01492333e-06 -9.12624600e-05 -6.95208582e-05 ... -1.95068016e-03\n",
            "  -2.04177151e-04 -3.32914316e-03]\n",
            " [-3.08979070e-05 -1.04486957e-04 -1.07569555e-04 ... -1.61629915e-03\n",
            "  -2.62537133e-03 -1.29018794e-03]\n",
            " [-3.27251109e-05 -7.08198640e-05 -8.63570822e-05 ...  3.88607878e-05\n",
            "  -1.65285121e-04 -2.28583557e-03]\n",
            " ...\n",
            " [ 3.56319040e-04  2.00601397e-04  2.51560356e-04 ... -1.48453831e-03\n",
            "  -4.54139517e-04 -1.28938770e-03]\n",
            " [ 8.74477613e-04  5.61608642e-04  5.86013339e-05 ...  3.00354790e-04\n",
            "  -1.24818194e-04  3.66766966e-04]\n",
            " [ 8.90090072e-04 -1.09047134e-04  3.34946380e-04 ... -6.69020868e-04\n",
            "   2.36629116e-04  4.46380553e-04]]\n",
            "Epoch number:  3\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[ 2.6253058e-04 -7.1554794e-05 -3.9801182e-04 ... -1.4973388e-03\n",
            "  -9.0457243e-04 -2.4587733e-03]\n",
            " [-8.9114241e-04 -1.3600971e-04 -9.8049536e-04 ... -1.1086731e-03\n",
            "  -1.7731467e-03 -9.5472741e-04]\n",
            " [-6.6468684e-04 -3.0462616e-04 -5.4465368e-04 ... -1.2526270e-03\n",
            "  -1.0428448e-03 -2.3831010e-03]\n",
            " ...\n",
            " [-7.8307779e-04 -8.2683895e-04 -9.7093545e-04 ... -2.8506250e-04\n",
            "   1.7765467e-04  5.0553936e-06]\n",
            " [-5.5906625e-04 -6.5134873e-04 -6.2852108e-04 ... -9.1467809e-05\n",
            "   1.7419005e-03  2.7128512e-03]\n",
            " [-6.6933734e-04 -5.7522039e-04 -8.4231119e-04 ... -1.6336137e-04\n",
            "   1.8257221e-03  2.9070855e-03]]\n",
            "Epoch number:  4\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[ 1.23951514e-03  4.59385192e-04 -1.19518474e-04 ...  2.10294485e-04\n",
            "   2.10285521e-04  2.10319413e-04]\n",
            " [-5.84275258e-05 -2.28442004e-04  1.32711604e-04 ...  2.10291037e-04\n",
            "   2.10295577e-04  2.10303944e-04]\n",
            " [-2.05655233e-05 -6.34002645e-05  1.46983832e-04 ...  2.10292201e-04\n",
            "   2.10301543e-04  2.10288141e-04]\n",
            " ...\n",
            " [ 2.06437107e-04  8.07119941e-05  8.72319069e-05 ...  7.20332609e-05\n",
            "  -1.83596669e-04  1.10422913e-03]\n",
            " [ 2.14144005e-04  2.44528812e-04  2.22218645e-04 ...  2.09683392e-04\n",
            "   1.96697656e-03  5.73323341e-03]\n",
            " [ 2.17730209e-04  3.26544046e-04  7.31258886e-04 ...  9.53284733e-04\n",
            "   3.59703740e-03  7.44329253e-03]]\n",
            "Epoch number:  5\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[-2.0712384e-04 -2.0711367e-04 -2.0715204e-04 ... -5.4561783e-04\n",
            "  -1.8923773e-04 -1.1315317e-03]\n",
            " [-2.0712941e-04 -2.0709750e-04 -2.0714159e-04 ... -2.4466059e-04\n",
            "  -6.8344350e-04 -8.2452904e-04]\n",
            " [-2.0714423e-04 -2.0710660e-04 -2.0713499e-04 ... -6.0683960e-06\n",
            "  -4.2770465e-05 -7.4478076e-04]\n",
            " ...\n",
            " [-2.7284815e-04 -1.6131706e-04 -1.6985275e-04 ... -4.4229461e-04\n",
            "  -3.8587922e-04 -3.1521131e-04]\n",
            " [-2.1604885e-04 -1.6190376e-04 -1.6779822e-04 ... -1.0174489e-04\n",
            "  -1.9854053e-04 -2.4460864e-04]\n",
            " [-2.3432875e-04 -2.1929762e-04 -2.4786632e-04 ... -4.7629219e-05\n",
            "  -2.6623998e-04 -1.9925027e-04]]\n",
            "Epoch number:  6\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[ 9.63388942e-04 -5.85748348e-05  2.33579907e-04 ...  8.43759859e-04\n",
            "   3.08179157e-03 -4.99170710e-05]\n",
            " [ 1.67922422e-04  3.31262068e-04  1.20156255e-04 ... -1.52898210e-04\n",
            "  -3.16821097e-04 -5.36873122e-04]\n",
            " [ 1.26458457e-04  3.28984199e-04  6.11365656e-04 ...  7.84085481e-04\n",
            "   2.98832078e-04 -1.02296658e-03]\n",
            " ...\n",
            " [ 5.15246647e-04  4.14745969e-04  1.75335517e-04 ... -9.24348086e-03\n",
            "  -1.93978916e-03 -2.46129069e-03]\n",
            " [ 6.48315181e-04  5.22368820e-04 -1.13156706e-03 ... -4.68997192e-03\n",
            "  -4.21053817e-04 -1.49028332e-04]\n",
            " [ 1.25154899e-03  7.31609005e-04 -1.45587209e-03 ... -5.48241846e-03\n",
            "  -3.22234398e-03  4.32316941e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eKLMOT1baLKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpIkFQH4SFbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_input=[]\n",
        "x_input = train_x_data[3400:3400+100]\n",
        "x_input = torch.Tensor(x_input).cuda()\n",
        "x_input = x_input.transpose(2,3).transpose(1,2)\n",
        "out, out_labels, out_layer, mus, variances = net(x_input)\n",
        "\n",
        "input_images = x_input.data.cpu().numpy()\n",
        "output_images = out.data.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mk1LHRx_X9EP",
        "colab_type": "code",
        "outputId": "7e7e66f2-13a1-43d3-bbae-9a52fe1d30f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "check=input_images[0][0]\n",
        "\n",
        "maximum=check[0][0]\n",
        "\n",
        "for i in range(len(check)):\n",
        "  for j in range(len(check[0])):\n",
        "    maximum=max(check[i][j],maximum)\n",
        "\n",
        "print(maximum)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9965801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sR8yLmz8SXwJ",
        "colab_type": "code",
        "outputId": "0682c392-8f71-4872-a1ef-a2e2b34d53d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1)\n",
        "# Hide grid lines\n",
        "ax.grid(False)\n",
        "\n",
        "# Hide axes ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.imshow(output_images[38][1], cmap='gray')\n",
        "fig.savefig('6_original.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8101988c50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZrYAUpgrxvEV",
        "colab_type": "code",
        "outputId": "e070c84e-6af9-41e0-b0bb-6c9c339de077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(input_images[71][1], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8102927320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f81029bf048>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}